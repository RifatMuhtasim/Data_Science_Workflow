{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jnzBy9AmnOjz"
      ],
      "authorship_tag": "ABX9TyOF05GC9HxjUXFyiHzHwUTo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RifatMuhtasim/Data_Science_Workflow/blob/main/1.1.Data_Load_And_Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the **Dataset**\n",
        "---"
      ],
      "metadata": {
        "id": "jnzBy9AmnOjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Dataset on Colab"
      ],
      "metadata": {
        "id": "Ol_0fAoyRlrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For Zip File\n",
        "\n",
        "import gdown\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Replace 'output_path' with the path where you want to save the file\n",
        "output_path = 'Robi_Datathon.zip'\n",
        "\n",
        "if os.path.exists(output_path):\n",
        "    print(\"File exists!\")\n",
        "\n",
        "else:\n",
        "    print(\"File does not exist.\")\n",
        "    # Replace 'file_id' with the ID of your file in Google Drive\n",
        "    file_id = '1fx0yBWwashiH2hODzEACgwacwqfd-Pq0'\n",
        "    gdown.download(f'https://drive.google.com/uc?id={file_id}', output_path, quiet=False)\n",
        "\n",
        "    # Path to your .zip file (Must Change. Same as the Output Path)\n",
        "    zip_file_path = '/content/Robi_Datathon.zip'\n",
        "\n",
        "    # Extract the contents of the .zip file to the root directory\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content/')\n",
        "\n",
        "    # List the contents of the root directory\n",
        "    extracted_files = !ls -a /content/\n",
        "    print(\"Files extracted to root directory:\", extracted_files)"
      ],
      "metadata": {
        "id": "uRl7iwUbRjUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Single File\n",
        "\n",
        "import gdown\n",
        "import os\n",
        "\n",
        "# Replace 'output_path' with the path where you want to save the file\n",
        "output_path = 'co2_emissions_train.csv'\n",
        "\n",
        "if os.path.exists(output_path):\n",
        "    print(\"File exists!\")\n",
        "else:\n",
        "    print(\"File does not exist.\")\n",
        "    # Replace 'file_id' with the ID of your file in Google Drive\n",
        "    file_id = '11zlCerdOfSBcTtFYHlz4DRgzjQeTGa8N'\n",
        "    gdown.download(f'https://drive.google.com/uc?id={file_id}', output_path, quiet=False)"
      ],
      "metadata": {
        "id": "K0x26AZhRk0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "\n",
        "# From csv\n",
        "df = pd.read_csv(\"/content/abcd.csv\")\n",
        "\n",
        "# From .tsv\n",
        "df = pd.read_csv(\"abcd.tsv\", delimiter=\"\\t\")\n",
        "\n",
        "# From excel\n",
        "df = pd.read_excel(\"abcd.xlsx\", sheet_name='sheet1')\n",
        "\n",
        "# From json\n",
        "df = pd.read_json(\"abcd.json\")"
      ],
      "metadata": {
        "id": "vbAZ9Xx9nJI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the dimensions of the dataset\n",
        "\n",
        "df.shape\n",
        "print(\"Number of Rows:\" , df.shape[0])\n",
        "print(\"Number of Columns:\", df.shape[1])"
      ],
      "metadata": {
        "id": "WvdTWBsDoJbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handle Missing Value\n",
        "---"
      ],
      "metadata": {
        "id": "rbjYlGI7Swkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check for data types and missing values"
      ],
      "metadata": {
        "id": "917NqucRpvjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect data types\n",
        "\n",
        "df.info()\n",
        "\n",
        "# or\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "_AT-dgBxpB_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify missing values in each column\n",
        "\n",
        "missing_values = df.isna().sum()\n",
        "\n",
        "# Determine the extent of missing values (as a percentage)\n",
        "missing_percentage = (missing_values / len(df)) * 100"
      ],
      "metadata": {
        "id": "8nONnE0IpD_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Dataframe to show the missing value with it's percentage\n",
        "\n",
        "total_missing_value = train.isna().sum().sort_values(ascending=False)\n",
        "percent_missing_value = train.isna().mean().sort_values(ascending=False) * 100\n",
        "missing_data = pd.concat([total_missing_value, percent_missing_value], axis=\"columns\", keys=['Total', 'Percent'])\n",
        "missing_data"
      ],
      "metadata": {
        "id": "IN1u8_gvex4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determine the appropriate strategy for handling missing values\n",
        "Based on the extent and nature of missing values, you can decide on strategies like:\n",
        "1. Dropping rows or columns with missing values\n",
        "2. Imputation (filling missing values with a specific value, e.g., mean, median, mode)\n",
        "  - Univariate:\n",
        "    - Numerical: Mean, Median, Mode, End of the Distribution\n",
        "    - Categorical: Mode, 'Missing'\n",
        "  - Multivariate:\n",
        "    - KNN Impute\n",
        "    - Iterative Impute\n",
        "3. Using interpolation methods\n",
        "4. Forward and Backward Technique\n",
        "5. Depending on the context, keeping missing values as is\n"
      ],
      "metadata": {
        "id": "NX638Uzbpy8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## # 1. Drop Columns and Rows"
      ],
      "metadata": {
        "id": "an4DqADDS44_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of dropping rows with missing values\n",
        "df.dropna(inplace=True)  # Drop rows with any missing values\n",
        "\n",
        "# Example of dropping columns with missing value\n",
        "df.dropna(axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "5QNhVuoqqv9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## # 2. Imputation"
      ],
      "metadata": {
        "id": "xuzyz5D5TF26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Univariate (Numerical)"
      ],
      "metadata": {
        "id": "KiOn_GpfS-Wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of imputation using mean\n",
        "df.fillna(df.mean(), inplace=True)  # Replace missing values with the mean of the column\n",
        "\n",
        "# Example of imputation using median\n",
        "df.fillna(df.median(), inplace=True)  # Replace missing values with the median of the column\n",
        "\n",
        "# Example of imputation using mode\n",
        "df.fillna(df.mode().iloc[0], inplace=True)  # Replace missing values with the mode of the column\n",
        "\n",
        "# End of the distribution\n",
        "lower_tail_value = df['column_name'].quantile(0.05)\n",
        "df['column_name'].fillna(lower_tail_value, inplace=True)"
      ],
      "metadata": {
        "id": "KTJI_86QpmO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  2. Univariate (Categorical)"
      ],
      "metadata": {
        "id": "PiDC4IHBTM9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of imputation using mode (for categorical data)\n",
        "df.fillna(df.mode().iloc[0], inplace=True)  # Replace missing values with the mode of the column\n",
        "\n",
        "# Example of fill missing value with \"Missing\" tag\n",
        "df['column_name'].fillna(\"Missing\", inplace=True)"
      ],
      "metadata": {
        "id": "eXcrnaKFtfZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Multivariate"
      ],
      "metadata": {
        "id": "uUF8j0v1TWFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### KNN Impute"
      ],
      "metadata": {
        "id": "0HwY0BvEUQBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "knn_imputer = KNNImputer(n_neighbors=5)\n",
        "\n",
        "features_dataset = dataset.drop(['label'], axis=\"columns\")\n",
        "label_dataset = dataset['label']\n",
        "features_dataset.iloc[:, :] = knn_imputer.fit_transform(features_dataset)\n",
        "dataset = pd.concat([features_dataset, label_dataset], axis=\"columns\")\n",
        "\n",
        "\n",
        "# Iterative Impute (MICE)\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "iterative_imputer = IterativeImputer(max_iter=500)\n",
        "\n",
        "features_dataset = dataset.drop(['label'], axis=\"columns\")\n",
        "label_dataset = dataset['label']\n",
        "features_dataset.iloc[:, :] = iterative_imputer.fit_transform(features_dataset)\n",
        "dataset = pd.concat([features_dataset, label_dataset], axis=\"columns\")"
      ],
      "metadata": {
        "id": "iaxhe4_umedm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Iterative Imputation"
      ],
      "metadata": {
        "id": "AD5U7vjOUc_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "def imputation(df):\n",
        "    imputer = IterativeImputer(missing_values=np.nan,\n",
        "                            random_state=0,\n",
        "                            n_nearest_features=3,\n",
        "                            max_iter=1,\n",
        "                            sample_posterior=True)\n",
        "\n",
        "    df_imp = imputer.fit_transform(df)\n",
        "    df = pd.DataFrame(df_imp, columns=df.columns.tolist())\n",
        "    return df\n",
        "\n",
        "# y = train_df['Class']\n",
        "# train_df.drop(columns='Class', inplace=True)\n",
        "train_num = imputation(train_df[[col for col in train_df.select_dtypes(\"number\")]])\n",
        "train_cat = train_df[[col for col in train_df.select_dtypes([\"category\", \"object\"])]]\n",
        "train_df = pd.concat([train_cat, train_num], axis=1)\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "HOJb6qT4-gm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## # 3. Iterpolation Technique"
      ],
      "metadata": {
        "id": "6WNTKwFqUvg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing value with Linear Interpolate\n",
        "new_df = df.interpolate(method=\"linear\")\n",
        "\n",
        "# Fill missing value with Time Interpolate\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df.set_index('date', inplace=True)\n",
        "new_df = df.interpolate(method=\"time\")"
      ],
      "metadata": {
        "id": "XuSfS7LeuHYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Forward & Backward Technique"
      ],
      "metadata": {
        "id": "YQXN76sIU6cN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled = df.fillna(method=\"ffill\")\n",
        "df_filled = df.fillna(method=\"bfill\")"
      ],
      "metadata": {
        "id": "OYPdSv1h3C3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Random Imputation"
      ],
      "metadata": {
        "id": "lwdEb6VBU-hO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate random values from the distribution of non-missing values\n",
        "non_missing_values = df['column_name'].dropna()\n",
        "random_values = np.random.choice(non_missing_values, size=df['column_name'].isnull().sum())\n",
        "df.loc[df['column_name'].isnull(), 'column_name'] = random_values"
      ],
      "metadata": {
        "id": "YezyGZIFuHR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Missing Indicator"
      ],
      "metadata": {
        "id": "Wwrq_zw-VHle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Missing Indication in columns\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "si_imputer = SimpleImputer(add_indicator=True)\n",
        "df.iloc[:, :] = si_imputer.fit_transform(df)"
      ],
      "metadata": {
        "id": "f4GeEhSfweFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It’s important to understand the reasons behind missing data:\n",
        "\n",
        "Identifying the type of missing data: Is it Missing Completely at Random (MCAR), Missing at Random (MAR), or Missing Not at Random (MNAR)?\n",
        "Evaluating the impact of missing data: Is the missingness causing bias or affecting the analysis?<br/>\n",
        "Choosing appropriate handling strategies: Different techniques are suitable for different types of missing data.<br/><br/>\n",
        "Types of Missing Values?<br/>\n",
        "There are three main types of missing values:?<br/>\n",
        "\n",
        "Missing Completely at Random (MCAR): MCAR is a specific type of missing data in which the probability of a data point being missing is entirely random and independent of any other variable in the dataset. In simpler terms, whether a value is missing or not has nothing to do with the values of other variables or the characteristics of the data point itself.\n",
        "\n",
        "\n",
        "Missing at Random (MAR): MAR is a type of missing data where the probability of a data point missing depends on the values of other variables in the dataset, but not on the missing variable itself. This means that the missingness mechanism is not entirely random, but it can be predicted based on the available information.\n",
        "\n",
        "\n",
        "Missing Not at Random (MNAR): MNAR is the most challenging type of missing data to deal with. It occurs when the probability of a data point being missing is related to the missing value itself. This means that the reason for the missing data is informative and directly associated with the variable that is missing.\n"
      ],
      "metadata": {
        "id": "zzQjNA_g5oYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handle Duplicates\n",
        "---"
      ],
      "metadata": {
        "id": "HXiyPD81Y4RA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## # Identify Duplicates"
      ],
      "metadata": {
        "id": "xut-DspbY8ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the number of Duplicates value\n",
        "\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "wI3WCJKiY6Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drop Duplicates"
      ],
      "metadata": {
        "id": "qz_Stc9zZF-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_no_duplicates = df.drop_duplicates()"
      ],
      "metadata": {
        "id": "9QIZRLpeZCMR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}