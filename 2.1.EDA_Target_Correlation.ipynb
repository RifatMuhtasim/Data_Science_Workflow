{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzIr+XtZb6fQm5R/OSOA8k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RifatMuhtasim/Data_Science_Workflow/blob/main/2.1.EDA_Target_Correlation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Explore Distribution of Target Variable"
      ],
      "metadata": {
        "id": "WNcNRbTI5o8U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzbO6_Z02Hhx"
      },
      "outputs": [],
      "source": [
        "# Visualize the distribution of the target variable\n",
        "# Draw a Histogram for CO2 Emissions\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.figure_factory as ff\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "\n",
        "def Histogram_plot_kde(df, x):\n",
        "    hist_data = df[x]\n",
        "    skewness = skew(hist_data)\n",
        "    kurtosis_value = kurtosis(hist_data)\n",
        "    fig = ff.create_distplot([hist_data], [x], curve_type='kde')\n",
        "    fig.update_layout(title= f\"{x} Distribution. Skewness: {round(skewness, 2)} and Kurtosis: {round(kurtosis_value, 2)}\")\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "Histogram_plot_kde(df, 'regression')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the target variable skewness and histogram\n",
        "\n",
        "import scipy.stats as stat\n",
        "from scipy.stats import skew, kurtosis\n",
        "import statsmodels.api as sm\n",
        "import pylab\n",
        "\n",
        "\n",
        "def Diagnostic_plot(df, x):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(x=df[x],  kde=True)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    stat.probplot(df[x], dist=\"norm\", plot = pylab)\n",
        "\n",
        "    print(\"Skew:\", skew(df[x]))\n",
        "    print(\"Kurtosis:\", kurtosis(df[x]))\n",
        "    plt.show()\n",
        "\n",
        "Diagnostic_plot(df, 'regression')"
      ],
      "metadata": {
        "id": "ltVmBv0JdCO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation to achieve to a symmetrical distribution\n",
        "\n",
        "train_copy = train.copy()\n",
        "train_copy['Log_CO2_Emissions'] = np.log(train['CO2 emissions (metric tons per capita)'])\n",
        "\n",
        "hist_data = train_copy['Log_CO2_Emissions']\n",
        "skewness = skew(hist_data)\n",
        "kurtosis_value = kurtosis(hist_data)\n",
        "fig = ff.create_distplot([hist_data], ['CO2 Emissions'], curve_type='kde')\n",
        "fig.update_layout(title= f\"C02 Emissions Distribution. Skewness: {round(skewness, 2)} and Kurtosis: {round(kurtosis_value, 2)}\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "glfIX5zC6dFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Visualize Relationship between features and Target"
      ],
      "metadata": {
        "id": "Yqj8y9sX7dBO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6B09_Trx7jGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ocRW9xWF8ki8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HMb0Kqcl8kbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Explore Correlations between Features"
      ],
      "metadata": {
        "id": "m6YEm2oO769Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute pairwise correlation"
      ],
      "metadata": {
        "id": "XlIclOB0koZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_df = df.select_dtypes(include=['int', 'float'])\n",
        "sns.pairplot(numerical_df)"
      ],
      "metadata": {
        "id": "umvMB99N7-gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Correlation using Heatmap"
      ],
      "metadata": {
        "id": "mDvVq5UVkrXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# When we have regression problem\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "numerical_df = train_df.select_dtypes(include=['int', 'float'])\n",
        "sns.heatmap(numerical_df.corr(), annot=True, fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IxBx9-_a8zst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identify Highly Correlated Columns"
      ],
      "metadata": {
        "id": "aW8fHxbbkwwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression"
      ],
      "metadata": {
        "id": "9_2hgcKrmLS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# When both are Continuous\n",
        "\n",
        "def Pearson_correlation_list(df, target):\n",
        "    df_copy = df.copy()\n",
        "    correlation_matrix = df_copy.corr()\n",
        "    correlation_matrix.reset_index(inplace=True)\n",
        "    correlation_df = correlation_matrix[['index', target]]\n",
        "    correlation_df = correlation_df[correlation_df['index'] != target]\n",
        "    return correlation_df.sort_values(\"value\", ascending=False)\n",
        "\n",
        "correlation_df = Pearson_correlation_list(df= df, target= 'regression')\n",
        "correlation_df"
      ],
      "metadata": {
        "id": "3rY85yQTs6IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def High_correlated(dataset, threshold):\n",
        "    high_correlated = []\n",
        "    correlation_matrix = dataset.corr()\n",
        "\n",
        "    for i in range(len(correlation_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if correlation_matrix.iloc[i, j] > threshold:\n",
        "                colname = correlation_matrix.columns[i]\n",
        "                high_correlated.append(colname)\n",
        "    return high_correlated\n",
        "\n",
        "\n",
        "high_correlated_features = High_correlated(df, 0.8)\n",
        "high_correlated_features"
      ],
      "metadata": {
        "id": "RxJl_nfJB53V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "not_remove = ['classification', 'classification',  'regression', 'regression', 'regression' ]\n",
        "print(\"Not Remove: \", not_remove)\n",
        "\n",
        "high_correlated_features = [item for item in high_correlated_features if item not in not_remove]\n",
        "high_correlated_features\n",
        "\n",
        "# Drop High Correlated Columns\n",
        "df = df.drop(high_correlated_features, axis=\"columns\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "IvmpXrrnk8kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification"
      ],
      "metadata": {
        "id": "fmfq1lAhnzyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Binary Classification"
      ],
      "metadata": {
        "id": "OlaCayuIn44A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#When Target is Binary and Features are continuous\n",
        "\n",
        "from scipy.stats import pointbiserialr\n",
        "\n",
        "def Pointbiserial(df, target):\n",
        "    df_copy = df.copy()\n",
        "    columns = [col for col in df_copy.columns]\n",
        "    correlation_df = pd.DataFrame(columns=['column', 'value'])\n",
        "\n",
        "    for i in columns:\n",
        "        correlation, p_value = pointbiserialr(df_copy[i], df_copy[target])\n",
        "        correlation_df = pd.concat([correlation_df, pd.DataFrame({'column': [i], 'value': [correlation]})], ignore_index=True)\n",
        "\n",
        "    correlation_df = correlation_df[correlation_df['column'] != target]\n",
        "    return correlation_df.sort_values(\"value\", ascending=False)\n",
        "\n",
        "\n",
        "correlation_df = Pointbiserial(numerical_df, target= 'classification')\n",
        "correlation_df"
      ],
      "metadata": {
        "id": "y-te2m0En1gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Categorical Classification"
      ],
      "metadata": {
        "id": "_Y8QFD87pcHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# When target are categorical and Features are continuous\n",
        "\n",
        "def Classification_correlation_ratio(df, target):\n",
        "    df_copy = df.copy()\n",
        "    def correlation_ratio(categories, measurements):\n",
        "        # Convert categorical labels to integers\n",
        "        categories = np.array(categories)\n",
        "        unique_categories = np.unique(categories)\n",
        "        category_counts = {category: np.sum(categories == category) for category in unique_categories}\n",
        "        # Total sum of squares\n",
        "        total_variance = np.var(measurements) * (len(measurements) - 1)\n",
        "\n",
        "        # Between-group sum of squares\n",
        "        numerator = 0\n",
        "        for category, count in category_counts.items():\n",
        "            category_measurements = measurements[categories == category]\n",
        "            numerator += count * np.var(category_measurements)\n",
        "\n",
        "        # Calculate correlation ratio\n",
        "        eta = numerator / total_variance\n",
        "        return eta\n",
        "\n",
        "    columns = [col for col in df_copy.columns]\n",
        "    correlation_df = pd.DataFrame(columns=['column', 'value'])\n",
        "    for i in columns:\n",
        "        correlation_eta = correlation_ratio(df_copy[target], df_copy[i])\n",
        "        correlation_df = pd.concat([correlation_df, pd.DataFrame({'column': [i], 'value': [correlation_eta]})], ignore_index=True)\n",
        "\n",
        "    correlation_df = correlation_df[correlation_df['column'] != target]\n",
        "    return correlation_df.sort_values(\"value\", ascending=False)"
      ],
      "metadata": {
        "id": "kJ4X9BWWqBQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_df = Classification_correlation_ratio(df= numerical_df, target= 'classification')\n",
        "correlation_df"
      ],
      "metadata": {
        "id": "L_ftrw9eptE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### When Both Features and Target are categorical"
      ],
      "metadata": {
        "id": "ApMit195qa0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# When both are Categorical\n",
        "\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "\n",
        "def CramersV(df, target):\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    def cramers_v(confusion_matrix):\n",
        "        chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "        n = confusion_matrix.sum()\n",
        "        r, k = confusion_matrix.shape\n",
        "        phi2 = chi2 / n\n",
        "        phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
        "        rcorr = r - ((r-1)**2)/(n-1)\n",
        "        kcorr = k - ((k-1)**2)/(n-1)\n",
        "        return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
        "\n",
        "\n",
        "    columns = [col for col in df_copy.columns]\n",
        "    correlation_df = pd.DataFrame(columns=['column', 'value'])\n",
        "    for i in columns:\n",
        "        conf_matrix = pd.crosstab(df_copy[i], df_copy[target])\n",
        "        # Calculate CramÃ©r's V\n",
        "        cramers_v_value = cramers_v(conf_matrix.values)\n",
        "        correlation_df = pd.concat([correlation_df, pd.DataFrame({'column': [i], 'value': [cramers_v_value]})], ignore_index=True)\n",
        "\n",
        "    correlation_df = correlation_df[correlation_df['column'] != target]\n",
        "    return correlation_df.sort_values(\"value\", ascending=False)"
      ],
      "metadata": {
        "id": "iHIsIUq4qZMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_df = CramersV(df= df, target='target')\n",
        "correlation_df"
      ],
      "metadata": {
        "id": "Y0xt-pXdqkxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Drop High Correlated Columns"
      ],
      "metadata": {
        "id": "YGhE5iPZqxXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_threshold = 0.90\n",
        "min_threshold = 0.10\n",
        "\n",
        "removed_columns = correlation_df[\n",
        "    (correlation_df['value'] > max_threshold) |\n",
        "    ((correlation_df['value'] < min_threshold ) & (correlation_df['value'] >= 0)) |\n",
        "    (correlation_df['value'].isna())\n",
        "]['column'].tolist()\n",
        "\n",
        "removed_columns"
      ],
      "metadata": {
        "id": "PTi8gglCq0_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(removed_columns, axis=\"columns\")"
      ],
      "metadata": {
        "id": "UJ59Yx0Sr2GC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}