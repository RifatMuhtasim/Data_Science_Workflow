{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFd9WJsPMXWPT4h4g9O+GO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RifatMuhtasim/Data_Science_Workflow/blob/main/6.1.Ensemble_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble Methods follow 4 Technique:\n",
        "1. Voting\n",
        "4. Stacking\n",
        "2. Bagging\n",
        "3. Boosting"
      ],
      "metadata": {
        "id": "NtLzxL7UXDtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Voting"
      ],
      "metadata": {
        "id": "Q5ld28LMXT9_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBG75YBhxe-7"
      },
      "outputs": [],
      "source": [
        "# Ensemble Method for Classification Voting\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize individual models\n",
        "model1 = DecisionTreeClassifier()\n",
        "model2 = SVC(probability=True)  # Ensure SVC is trained with probability=True for soft voting\n",
        "model3 = LogisticRegression()\n",
        "\n",
        "# For hard voting, you don't need `voting='soft'` argument\n",
        "ensemble_model = VotingClassifier(estimators=[('dt', model1), ('svm', model2), ('lr', model3)], voting='soft')\n",
        "\n",
        "# You can also set weights for individual models if some are more important than others\n",
        "# ensemble_model = VotingClassifier(estimators=[('dt', model1), ('svm', model2), ('lr', model3)], voting='soft', weights=[2,1,1])\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "scores = cross_val_score(ensemble_model, X_train, y_train, cv=5)\n",
        "print(\"Cross-validation scores:\", scores)\n",
        "print(\"Mean accuracy:\", scores.mean())\n",
        "\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "predictions = ensemble_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy: \", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensemble Method for Regression Voting\n",
        "\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the individual regression models\n",
        "model1 = LinearRegression()\n",
        "model2 = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "model3 = SVR(kernel='linear')\n",
        "\n",
        "# Create the voting regressor ensemble\n",
        "voting_regressor = VotingRegressor(estimators=[\n",
        "    ('lr', model1),\n",
        "    ('rf', model2),\n",
        "    ('svr', model3)\n",
        "])\n",
        "\n",
        "voting_regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = voting_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the ensemble\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)"
      ],
      "metadata": {
        "id": "raZDNNf3nKdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Stacking"
      ],
      "metadata": {
        "id": "Im3et_x2n8BV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacking for Classification\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define your base estimators\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=10)),\n",
        "    ('gbdt', GradientBoostingClassifier())\n",
        "]\n",
        "\n",
        "# Define your stacking classifier with logistic regression as the final estimator\n",
        "clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Train the stacking classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "MYWHbdPIvB9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacking for Regression\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Define your base estimators\n",
        "estimators = [\n",
        "    ('rf', RandomForestRegressor(n_estimators=10, random_state=42)),\n",
        "    ('knn', KNeighborsRegressor(n_neighbors=10)),\n",
        "    ('gbdt', GradientBoostingRegressor())\n",
        "]\n",
        "\n",
        "# Define your stacking regressor with linear regression as the final estimator\n",
        "reg = StackingRegressor(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LinearRegression()\n",
        ")\n",
        "\n",
        "# Train the stacking regressor\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "# Calculate mean squared error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error: \", mse)\n"
      ],
      "metadata": {
        "id": "zsuf8qMJvgM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Bagging"
      ],
      "metadata": {
        "id": "1bfM96mty0pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baggin Technique for Classification (SVM)\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Initialize base classifier\n",
        "base_classifier = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# Initialize bagging classifier\n",
        "bagging_classifier = BaggingClassifier(base_estimator=base_classifier, n_estimators=10)\n",
        "\n",
        "# Evaluate bagging classifier using cross-validation\n",
        "scores = cross_val_score(bagging_classifier, X_train, y_train, cv=5)\n",
        "print(\"Cross-validation scores:\", scores)\n",
        "print(\"Mean accuracy:\", scores.mean())\n",
        "\n",
        "# Train bagging classifier on the full dataset\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = bagging_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "r8NNYUFhvhqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bagging Technique for Regression (Linear Regression)\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Initialize base regressor\n",
        "base_regressor = LinearRegression()\n",
        "\n",
        "# Initialize bagging regressor\n",
        "bagging_regressor = BaggingRegressor(base_estimator=base_regressor, n_estimators=10)\n",
        "\n",
        "# Evaluate bagging regressor using cross-validation\n",
        "scores = cross_val_score(bagging_regressor, X_train, y_train, cv=5)\n",
        "print(\"Cross-validation scores:\", scores)\n",
        "print(\"Mean accuracy:\", scores.mean())\n",
        "\n",
        "# Train bagging regressor on the full dataset\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = bagging_regressor.predict(X_test)\n",
        "\n",
        "\n",
        "# Calculate mean squared error (MSE)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(\"Mean Squared Error: \", mse)"
      ],
      "metadata": {
        "id": "KaLg0pLxzAZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Boosting"
      ],
      "metadata": {
        "id": "q2Y53b5HzsMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Initialize base classifier\n",
        "base_classifier = SVC(kernel='linear', probability=True)  # SVM with linear kernel\n",
        "\n",
        "# Initialize Gradient Boosting classifier\n",
        "gradient_boosting_classifier = GradientBoostingClassifier(base_estimator=base_classifier, n_estimators=50, random_state=42)\n",
        "\n",
        "# Evaluate Gradient Boosting classifier using cross-validation\n",
        "scores = cross_val_score(gradient_boosting_classifier, X_train, y_train, cv=5)\n",
        "print(\"Cross-validation scores:\", scores)\n",
        "print(\"Mean accuracy:\", scores.mean())\n",
        "\n",
        "# Train Gradient Boosting classifier on the full dataset\n",
        "gradient_boosting_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = gradient_boosting_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "ndkcELwAztA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Initialize base regressor\n",
        "# base_regressor = SVR(kernel='linear')  # SVR with linear kernel\n",
        "base_regressor = LinearRegression()\n",
        "\n",
        "# Initialize Gradient Boosting regressor\n",
        "gradient_boosting_regressor = GradientBoostingRegressor(base_estimator=base_regressor, n_estimators=50, random_state=42)\n",
        "\n",
        "# Evaluate Gradient Boosting regressor using cross-validation\n",
        "scores = cross_val_score(gradient_boosting_regressor, X_train, y_train, cv=5)\n",
        "print(\"Cross-validation scores:\", scores)\n",
        "print(\"Mean R^2 score:\", scores.mean())\n",
        "\n",
        "# Train Gradient Boosting regressor on the full dataset\n",
        "gradient_boosting_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = gradient_boosting_regressor.predict(X_test)\n",
        "\n",
        "# Calculate mean squared error (MSE)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(\"Mean Squared Error: \", mse)"
      ],
      "metadata": {
        "id": "R6FYWk6X1mPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Gradient Boosting ML Algo"
      ],
      "metadata": {
        "id": "KH4VNqYB6t9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Light GBM"
      ],
      "metadata": {
        "id": "GqPXvJisH-i5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Light GBM Regressor\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X = df.drop(['labels'], axis=\"columns\")\n",
        "y = df['labels']\n",
        "\n",
        "obj_feat = list(X.loc[:, X.dtypes=\"object\"].columns.values)\n",
        "for feature in obj_feat:\n",
        "    X[feature] = pd.Series(X[feature], dtype=\"category\")\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "lgbm = LGBMRegressor()\n",
        "lgbm.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = lgbm.predict(X_train)\n",
        "y_val_pred = lgbm.predict(X_val)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "val_mse = mean_squared_error(y_val, y_val_pred)\n",
        "\n",
        "# Calculate R-Squared Score\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "val_r2 = r2_score(y_val, y_val_pred )\n",
        "\n",
        "# Calculate RMSE\n",
        "train_rmse = math.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "val_rmse = math.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "\n",
        "print(\"Light GBM: \")\n",
        "print(\"Train R-squared:\", train_r2)\n",
        "print(\"Validation R-squared:\", val_r2)\n",
        "print(\"Train RMSE:\", train_rmse)\n",
        "print(\"Validation RMSE:\", val_rmse)"
      ],
      "metadata": {
        "id": "eGFmDddRQjUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Light GBM Classifier\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X = df.drop(['labels'], axis=\"columns\")\n",
        "y = df['labels']\n",
        "\n",
        "obj_feat = list(X.loc[:, X.dtypes=\"object\"].columns.values)\n",
        "for feature in obj_feat:\n",
        "    X[feature] = pd.Series(X[feature], dtype=\"category\")\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "lgbm = LGBMClassifier()\n",
        "lgbm.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = lgbm.predict(X_train)\n",
        "y_val_pred = lgbm.predict(X_val)\n",
        "\n",
        "# Calculate Accuracy\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "print(\"LightGBM: \")\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Validation Accuracy:\", val_accuracy)"
      ],
      "metadata": {
        "id": "4tkh-XGMQrlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CatBoost"
      ],
      "metadata": {
        "id": "xTRC6vbdRUGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CatBoost Regressor\n",
        "\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "\n",
        "# Constructing the pools\n",
        "pool_train = Pool(X_train, y_train, cat_features=['Country', 'Status', 'Year'])\n",
        "pool_val = Pool(X_val, y_val, cat_features=['Country', 'Status', 'Year'])  # Include y_val\n",
        "\n",
        "# Model training\n",
        "cbr = CatBoostRegressor()\n",
        "cbr.fit(pool_train)\n",
        "\n",
        "# Prediction\n",
        "y_train_pred = cbr.predict(pool_train)\n",
        "y_val_pred = cbr.predict(pool_val)  # Use pool_val\n",
        "\n",
        "# Metrics Calculation\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "val_mse = mean_squared_error(y_val, y_val_pred)\n",
        "\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "val_r2 = r2_score(y_val, y_val_pred)\n",
        "\n",
        "train_rmse = math.sqrt(train_mse)\n",
        "val_rmse = math.sqrt(val_mse)\n",
        "\n",
        "print(\"CatBoost Regressor: \")\n",
        "print(\"Train R-squared:\", train_r2)\n",
        "print(\"Validation R-squared:\", val_r2)\n",
        "print(\"Train RMSE:\", train_rmse)\n",
        "print(\"Validation RMSE:\", val_rmse)"
      ],
      "metadata": {
        "id": "Kbl89CB1RVeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CatBoost Classifier\n",
        "\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Constructing the pools\n",
        "pool_train = Pool(X_train, y_train, cat_features=['Country', 'Status', 'Year'])\n",
        "pool_test = Pool(X_test, cat_features=['Country', 'Status', 'Year'])\n",
        "\n",
        "# Model training\n",
        "cbc = CatBoostClassifier()\n",
        "cbc.fit(pool_train)\n",
        "\n",
        "# Prediction\n",
        "y_train_pred = cbc.predict(pool_train)\n",
        "y_test_pred = cbc.predict(pool_test)\n",
        "\n",
        "# Evaluation\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"CatBoost Classifier: \")\n",
        "print(\"Train Accuracy:\", train_accuracy)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "FYiMXj9ITFUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning"
      ],
      "metadata": {
        "id": "QX0b-ZIZWS60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "def Hyperparameter_tuning(X_train, y_train, model_params):\n",
        "  scores = []\n",
        "\n",
        "  for model, mp in model_params.items():\n",
        "    cv = GridSearchCV(mp['model'], mp['params'], cv=5,  return_train_score=False)\n",
        "    cv.fit(X_train, y_train)\n",
        "    scores.append({\n",
        "      \"Model\": model,\n",
        "      \"Best_score\": cv.best_score_,\n",
        "      \"Best_params\": cv.best_params_\n",
        "    })\n",
        "\n",
        "  df = pd.DataFrame(scores, columns=['Model', 'Best_score', 'Best_params'])\n",
        "  return df"
      ],
      "metadata": {
        "id": "vb57vrF4WVzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_params = {\n",
        "    'light_gbm': {\n",
        "        'model': LGBMClassifier(),\n",
        "        'params': {\n",
        "            'learning_rate': [0.01, 0.05, 0.1],\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'num_leaves': [20, 30, 40],\n",
        "            'max_depth': [5, 10, 15],\n",
        "            'min_child_samples': [20, 30, 40],\n",
        "            'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "            'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "            'reg_alpha': [0.0, 0.1, 0.5, 1.0],\n",
        "            'reg_lambda': [0.0, 0.1, 0.5, 1.0]\n",
        "        }\n",
        "    },\n",
        "    'decision_tree':{\n",
        "        'model': CatBoostClassifier(),\n",
        "        'params': {\n",
        "            'iterations': [100, 200, 300],                 # Number of trees in the model\n",
        "            'learning_rate': [0.01, 0.05, 0.1],            # Step size shrinkage used in update to prevent overfitting\n",
        "            'depth': [4, 6, 8],                            # Depth of the trees\n",
        "            'l2_leaf_reg': [1, 3, 5],                      # L2 regularization coefficient\n",
        "            'random_strength': [0.1, 0.5, 1],              # Random strength parameter to make the model more robust to overfitting\n",
        "            'bagging_temperature': [0.1, 0.5, 1],          # Controls intensity of the Bayesian bootstrap method\n",
        "            'border_count': [32, 64, 128],                 # Number of splits for numerical features\n",
        "            'scale_pos_weight': [1, 2, 5]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "results = Hyperparameter_tuning(X_train, y_train, model_params)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "results"
      ],
      "metadata": {
        "id": "fZPsY9dgWpCK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}