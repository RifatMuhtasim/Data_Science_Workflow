{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjrcgwcUtnzw13rFeGEOgR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RifatMuhtasim/Data_Science_Workflow/blob/main/3.1.Create_New_Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polynomial Feature"
      ],
      "metadata": {
        "id": "X1cRhekX7fwP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHkWgWhU6uCh"
      },
      "outputs": [],
      "source": [
        "def Polynomial_features(df, target):\n",
        "    result_df = df.copy()\n",
        "    column_list = [col for col in df.columns]\n",
        "    column_list.remove(target)\n",
        "\n",
        "    for i in column_list:\n",
        "        result_df[i + '_square'] = df[i] ** 2\n",
        "        result_df[i + '_cube'] = df[i] ** 3\n",
        "        result_df[i + '_sqrt'] = np.sqrt(df[i])\n",
        "        result_df[i + '_log'] = np.log(df[i])\n",
        "    return result_df\n",
        "\n",
        "\n",
        "pdf = Polynomial_features(df=df, target=target)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interaction Features"
      ],
      "metadata": {
        "id": "86nXCAm777If"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Interaction_features_for_multiple_columns(df, target):\n",
        "    result_df = df.copy()\n",
        "    column_list = [col for col in df.columns]\n",
        "    column_list.remove(target)\n",
        "\n",
        "    for i in column_list:\n",
        "        for col in column_list:\n",
        "            if i not in col:\n",
        "                result_df[i + '+' + col] = df[i] + df[col]\n",
        "                result_df[i + '-' + col] = df[i] - df[col]\n",
        "                result_df[i + '*' + col] = df[i] * df[col]\n",
        "                result_df[i + '/' + col] = df[i] / df[col]\n",
        "    return result_df\n",
        "\n",
        "\n",
        "idf = Interaction_features_for_multiple_columns(df=pdf, target= 'target')"
      ],
      "metadata": {
        "id": "1A3NlaoU77gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binning or Discretization"
      ],
      "metadata": {
        "id": "b9hMitTN8UNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "# Define number of bins\n",
        "n_bins = 6\n",
        "kbins = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
        "binned_data = kbins.fit_transform(np.array(mdf['sepal length (cm)']).reshape(-1, 1))\n",
        "mdf['sepal area bins'] = binned_data\n",
        "mdf['sepal area bins'] = mdf['sepal area bins'].astype(int)"
      ],
      "metadata": {
        "id": "0iO3P7Gz8UzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove Features With Correlation Value"
      ],
      "metadata": {
        "id": "dPS9u1MH8ddj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Pearson Correlation: When Target and Features both are Numerical.\n",
        "- Point-Biserial Correlation: When Target is Binary and Features are Numerical.\n",
        "- Correlation Ratio: When Target are categorical and Features are Numerical.\n",
        "- Cramer's V: When Target are categorical and Features are Categorical."
      ],
      "metadata": {
        "id": "wAj43oM5-BXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## # Pearson Correlation"
      ],
      "metadata": {
        "id": "UB1djxlmNS5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Pearson_correlation_list(df, target):\n",
        "    df_copy = df.copy()\n",
        "    correlation_matrix = df_copy.corr()\n",
        "    correlation_matrix.reset_index(inplace=True)\n",
        "    correlation_df = correlation_matrix[['index', target]]\n",
        "    correlation_df = correlation_df[correlation_df['index'] != target]\n",
        "    correlation_df[target] = abs(correlation_df[target])\n",
        "    return correlation_df"
      ],
      "metadata": {
        "id": "Ri0MwBoPAuAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_df = Pearson_correlation_list(df= df, target= 'target')\n",
        "correlation_df.sort_values(\"target\", ascending=True)"
      ],
      "metadata": {
        "id": "CebxgcJH_zJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "removed_columns = correlation_df[(correlation_df['target'] > 0.9) | (correlation_df['target'] < 0.1) | (correlation_df['target'].isna())]['index'].tolist()\n",
        "df= df.drop(removed_columns, axis=\"columns\")"
      ],
      "metadata": {
        "id": "pn5Fk5qa_qrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## # Point-Biserial Correlation"
      ],
      "metadata": {
        "id": "B7Xe0EFHNYIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pointbiserialr\n",
        "\n",
        "def Pointbiserial(df, target):\n",
        "    df_copy = df.copy()\n",
        "    columns = [col for col in df_copy.columns]\n",
        "    correlation_df = pd.DataFrame(columns=['column', 'value'])\n",
        "\n",
        "    for i in columns:\n",
        "        correlation, p_value = pointbiserialr(df_copy[i], df_copy[target])\n",
        "        correlation_df = pd.concat([correlation_df, pd.DataFrame({'column': [i], 'value': [correlation]})], ignore_index=True)\n",
        "\n",
        "    correlation_df = correlation_df[correlation_df['column'] != target]\n",
        "    return correlation_df"
      ],
      "metadata": {
        "id": "Vjv_a9O2Im_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_df = Pointbiserial(df= df, target= 'target')\n",
        "correlation_df.sort_values(\"value\", ascending=True)"
      ],
      "metadata": {
        "id": "2Ip4mY1pAOBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "removed_columns = correlation_df[(correlation_df['value'] > 0.95) | (correlation_df['value'] < 0.05) | (correlation_df['value'].isna())]['column'].tolist()\n",
        "df = df.drop(removed_columns, axis=\"columns\")"
      ],
      "metadata": {
        "id": "4ToxxoWsAevR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## # Correlation Ratio"
      ],
      "metadata": {
        "id": "EbX-OcDHNaTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Classification_correlation_ratio(df, target):\n",
        "    df_copy = df.copy()\n",
        "    def correlation_ratio(categories, measurements):\n",
        "        # Convert categorical labels to integers\n",
        "        categories = np.array(categories)\n",
        "        unique_categories = np.unique(categories)\n",
        "        category_counts = {category: np.sum(categories == category) for category in unique_categories}\n",
        "        # Total sum of squares\n",
        "        total_variance = np.var(measurements) * (len(measurements) - 1)\n",
        "\n",
        "        # Between-group sum of squares\n",
        "        numerator = 0\n",
        "        for category, count in category_counts.items():\n",
        "            category_measurements = measurements[categories == category]\n",
        "            numerator += count * np.var(category_measurements)\n",
        "\n",
        "        # Calculate correlation ratio\n",
        "        eta = numerator / total_variance\n",
        "        return eta\n",
        "\n",
        "    columns = [col for col in df_copy.columns]\n",
        "    correlation_df = pd.DataFrame(columns=['column', 'value'])\n",
        "    for i in columns:\n",
        "        correlation_eta = correlation_ratio(df_copy[target], df_copy[i])\n",
        "        correlation_df = pd.concat([correlation_df, pd.DataFrame({'column': [i], 'value': [correlation_eta]})], ignore_index=True)\n",
        "\n",
        "    correlation_df = correlation_df[correlation_df['column'] != target]\n",
        "    return correlation_df"
      ],
      "metadata": {
        "id": "B85A1aiABMoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_df = Classification_correlation_ratio(df= df, target= 'target')\n",
        "correlation_df.sort_values(\"value\", ascending=True)"
      ],
      "metadata": {
        "id": "EMmPRdoiAsha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "removed_columns = correlation_df[(correlation_df['value'] > 0.95) | (correlation_df['value'] < 0.05) | (correlation_df['value'].isna())]['column'].tolist()\n",
        "df = df.drop(removed_columns, axis=\"columns\")"
      ],
      "metadata": {
        "id": "7ezR1PSPAyxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## # Cramer's V"
      ],
      "metadata": {
        "id": "PO58edhSNcgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "\n",
        "def CramersV(df, target):\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    def cramers_v(confusion_matrix):\n",
        "        chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "        n = confusion_matrix.sum()\n",
        "        r, k = confusion_matrix.shape\n",
        "        phi2 = chi2 / n\n",
        "        phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
        "        rcorr = r - ((r-1)**2)/(n-1)\n",
        "        kcorr = k - ((k-1)**2)/(n-1)\n",
        "        return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
        "\n",
        "\n",
        "    columns = [col for col in df_copy.columns]\n",
        "    correlation_df = pd.DataFrame(columns=['column', 'value'])\n",
        "    for i in columns:\n",
        "        conf_matrix = pd.crosstab(df_copy[i], df_copy[target])\n",
        "        # Calculate CramÃ©r's V\n",
        "        cramers_v_value = cramers_v(conf_matrix.values)\n",
        "        correlation_df = pd.concat([correlation_df, pd.DataFrame({'column': [i], 'value': [cramers_v_value]})], ignore_index=True)\n",
        "\n",
        "    correlation_df = correlation_df[correlation_df['column'] != target]\n",
        "    return correlation_df"
      ],
      "metadata": {
        "id": "kQ8wxG12A4Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_df = CramersV(df= df, target='target')\n",
        "correlation_df.sort_values(\"value\", ascending=True)"
      ],
      "metadata": {
        "id": "lhhEXDLAA9u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "removed_columns = correlation_df[(correlation_df['value'] > 0.95) | (correlation_df['value'] < 0.05) | (correlation_df['value'].isna())]['column'].tolist()\n",
        "df = df.drop(removed_columns, axis=\"columns\")"
      ],
      "metadata": {
        "id": "meCteFjkA9kV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}