{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvw2Tk6g8olxiUnfl0b99D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RifatMuhtasim/Data_Science_Workflow/blob/main/5.2.Optuna_Classification_Hyperparameter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrtWPr-T2NC6"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import optuna\n",
        "except:\n",
        "    !pip install --quiet optuna\n",
        "    import optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a simple scikit-learn model"
      ],
      "metadata": {
        "id": "TF3dCs032pPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple scikit-learn model\n",
        "\n",
        "# Single Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def objective(X, y):\n",
        "    clf = LogisticRegression()  # Define the model.\n",
        "\n",
        "    return cross_val_score(\n",
        "        clf, X, y, n_jobs=-1, cv=5\n",
        "    ).mean()  # Train and evaluate the model.\n",
        "\n",
        "\n",
        "result = objective(X_train, y_train)\n",
        "print(f\"Accuracy: {result}\")"
      ],
      "metadata": {
        "id": "kA5DM8te2hDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "def objective(X, y, model):\n",
        "    clf = model\n",
        "\n",
        "    return cross_val_score(\n",
        "        clf, X, y, n_jobs=-1, cv=5\n",
        "    ).mean()\n",
        "\n",
        "\n",
        "models = {'Logistic_Regression':  LogisticRegression(),\n",
        "                    'Random_Forest': RandomForestClassifier(),\n",
        "                    'Decision_Tree': DecisionTreeClassifier(),\n",
        "                    'XGB_Classifier': XGBClassifier(),\n",
        "                    'SVM': SVC(),\n",
        "                    'K_Nearest_Neighbors': KNeighborsClassifier(),\n",
        "                    'GaussianNB': GaussianNB()}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    result = objective(X_train, y_train, model=model)\n",
        "    print(f\"{model_name} Accuracy is: \", result)"
      ],
      "metadata": {
        "id": "p0OChMIg2uZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Optuna"
      ],
      "metadata": {
        "id": "yxnsdOiM20Uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna For Logistic Regression\n",
        "\n",
        "import optuna\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def LogisticRegression_Optuna(X, y):\n",
        "    def objective(trial, X, y):\n",
        "        params = {\n",
        "            'penalty' : trial.suggest_categorical('penalty', ['l1', 'l2']),\n",
        "            'C': trial.suggest_float('C', 1e-5, 1e5, log=True),\n",
        "            'solver': trial.suggest_categorical('solver', ['liblinear', 'saga'])\n",
        "        }\n",
        "\n",
        "        clf = LogisticRegression(**params)\n",
        "        return cross_val_score(clf, X, y, n_jobs=-1, cv=5).mean()\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(lambda trial: objective(trial, X, y), n_trials=100)\n",
        "    trial = study.best_trial\n",
        "    return trial\n",
        "\n",
        "result = LogisticRegression_Optuna(X_train, y_train)\n",
        "print(f\"Logistic Regression Accuracy: {result.value}\")\n",
        "print(f\"Best Hyperparameters: {result.params}\")"
      ],
      "metadata": {
        "id": "dhR1jS852x76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna For XGBoost\n",
        "\n",
        "import optuna\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def XGB_Optuna(X, y):\n",
        "    def objective(trial, X, y):\n",
        "        params = {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "            'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
        "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n",
        "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
        "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1\n",
        "        }\n",
        "\n",
        "        clf = XGBClassifier(**params)\n",
        "        return cross_val_score(clf, X, y, n_jobs=-1, cv=5).mean()\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(lambda trial: objective(trial=trial, X=X, y=y), n_trials=100)\n",
        "    trial = study.best_trial\n",
        "    return trial\n",
        "\n",
        "\n",
        "result = XGB_Optuna(X_train, y_train)\n",
        "print(f\"XGBoost Accuracy: {result.value}\")\n",
        "print(f\"Best Hyperparameters: {result.params}\")"
      ],
      "metadata": {
        "id": "2d0qlfXo26tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna For RandomForest\n",
        "\n",
        "import optuna\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def RandomForest_Optuna(X, y):\n",
        "    def objective(trial, X, y):\n",
        "        params = {\n",
        "            'n_estimators': trial.suggest_int(\"n_estimators\", 2, 20),\n",
        "            'max_depth': int(trial.suggest_float(\"max_depth\", 1, 32, log=True))\n",
        "        }\n",
        "        clf = RandomForestClassifier(**params)\n",
        "        return cross_val_score(clf, X, y, n_jobs=-1, cv=5).mean()\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(lambda trial: objective(trial= trial, X= X, y= y), n_trials=100)\n",
        "    trial = study.best_trial\n",
        "    return trial\n",
        "\n",
        "result = RandomForest_Optuna(X=X_train, y=y_train)\n",
        "print(f\"Random Forest Accuracy: {result.value}\")\n",
        "print(f\"Best Hyperparameters: {result.params}\")"
      ],
      "metadata": {
        "id": "DU7uosdj285r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna For DecisionTreeClassifier\n",
        "\n",
        "import optuna\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def DecisionTree_Optuna(X, y):\n",
        "    def objective(trial, X, y):\n",
        "        params = {\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
        "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        clf = DecisionTreeClassifier(**params)\n",
        "        return cross_val_score(clf, X, y, cv=5).mean()\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(lambda trial: objective(trial=trial, X=X, y=y), n_trials=100)\n",
        "    trial = study.best_trial\n",
        "    return trial\n",
        "\n",
        "\n",
        "result = DecisionTree_Optuna(X=X_train, y=y_train)\n",
        "print(f\"DecisionTree Accuracy: {result.value}\")\n",
        "print(f\"Best Hyperparameters: {result.params}\")"
      ],
      "metadata": {
        "id": "IlS_sIF92_tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna For SVM\n",
        "\n",
        "import optuna\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def SVC_Optuna(X, y):\n",
        "    def objective(trial, X, y):\n",
        "        params = {\n",
        "            'C': trial.suggest_float('C', 1e-5, 100, log=True),\n",
        "            'kernel': trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly', 'sigmoid']),\n",
        "            'gamma': trial.suggest_categorical('gamma', ['scale', 'auto']),\n",
        "            'degree': trial.suggest_int('degree', 1, 5),\n",
        "            'coef0': trial.suggest_float('coef0', 0.0, 10.0),\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        clf = SVC(**params)\n",
        "        return cross_val_score(clf, X, y, cv=5).mean()\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(lambda trial: objective(trial=trial, X=X, y=y), n_trials=100)\n",
        "    trial = study.best_trial\n",
        "    return trial\n",
        "\n",
        "result = SVC_Optuna(X=X_train, y=y_train)\n",
        "print(f\"SVC Accuracy: {result.value}\")\n",
        "print(f\"Best Hyperparameters: {result.params}\")"
      ],
      "metadata": {
        "id": "5PYxDckl3BvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna For KNeighborsClassifer\n",
        "\n",
        "import optuna\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def KNeighbors_Optuna(X, y):\n",
        "    def objective(trial, X, y):\n",
        "        params = {\n",
        "            'n_neighbors': trial.suggest_int('n_neighbors', 1, 30),\n",
        "            'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
        "            'algorithm': trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute']),\n",
        "            'leaf_size': trial.suggest_int('leaf_size', 10, 50),\n",
        "            'p': trial.suggest_int('p', 1, 2),\n",
        "            'n_jobs': -1\n",
        "        }\n",
        "\n",
        "        clf = KNeighborsClassifier(**params)\n",
        "        return cross_val_score(clf, X, y, cv=5).mean()\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(lambda trial: objective(trial=trial, X=X, y=y), n_trials=100)\n",
        "    trial = study.best_trial\n",
        "    return trial\n",
        "\n",
        "result = KNeighbors_Optuna(X=X_train, y=y_train)\n",
        "print(f\"KNeighborsClassifier Accuracy: {result.value}\")\n",
        "print(f\"Best Hyperparameters: {result.params}\")"
      ],
      "metadata": {
        "id": "unOtaJFI3Dn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def GaussianNB_Optuna(X, y):\n",
        "    def objective(trial, X, y):\n",
        "        params= {\n",
        "            'var_smoothing' : trial.suggest_float('var_smoothing', 1e-12, 1e-3, log=True)\n",
        "        }\n",
        "\n",
        "        clf = GaussianNB(**params)\n",
        "        return cross_val_score(clf, X, y, cv=5).mean()\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(lambda trial: objective(trial=trial, X=X, y=y), n_trials=100)\n",
        "    trial = study.best_trial\n",
        "    return trial\n",
        "\n",
        "\n",
        "result = GaussianNB_Optuna(X=X_train, y=y_train)\n",
        "print(f\"GaussianNB Accuracy: {result.value}\")\n",
        "print(f\"Best Hyperparameters: {result.params}\")"
      ],
      "metadata": {
        "id": "XhneoZ3g3Fuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mutlinomial NB\n",
        "\n",
        "import optuna\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def MultinomialNB_Optuna(X, y):\n",
        "    def objective(trial, X, y):\n",
        "        params = {\n",
        "            'alpha' : trial.suggest_float('alpha', 1e-5, 1.0, log=True)\n",
        "        }\n",
        "\n",
        "        clf = MultinomialNB(**params)\n",
        "        return cross_val_score(clf, X, y, cv=5).mean()\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(lambda trial: objective(trial=trial, X=X, y=y), n_trials=100)\n",
        "    trial = study.best_trial\n",
        "    return trial\n",
        "\n",
        "result = MultinomialNB_Optuna(X=X_train, y=y_train)\n",
        "print(f\"MultinomialNB Accuracy: {result.value}\")\n",
        "print(f\"Best Hyperparameters: {result.params}\")"
      ],
      "metadata": {
        "id": "fS-S_nA33IAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Multiple Optuna Model"
      ],
      "metadata": {
        "id": "wwNnK7xe3N2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Optuna on Multiple Model\n",
        "\n",
        "import optuna\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def Optuna_Hyperparameter_tuning(X, y):\n",
        "    def objective(trial, X, y):\n",
        "        classifier = trial.suggest_categorical(\"classifier\", ['RandomForest', \"XGB\"])\n",
        "\n",
        "        if classifier == \"RandomForest\":\n",
        "            rf_params = {\n",
        "                'n_estimators' : trial.suggest_int(\"rf_n_estimators\", 2, 20),\n",
        "                'max_depth': int(trial.suggest_float(\"rf_max_depth\", 3, 10)),\n",
        "                'random_state': 42,\n",
        "                'n_jobs': -1\n",
        "            }\n",
        "            clf = RandomForestClassifier(**rf_params)\n",
        "\n",
        "        else:\n",
        "            xgb_params = {\n",
        "                'n_estimators': trial.suggest_int('xgb_n_estimators', 100, 1000),\n",
        "                'max_depth': trial.suggest_int('xgb_max_depth', 3, 10),\n",
        "                'learning_rate': trial.suggest_float('xgb_learning_rate', 0.01, 0.3, log=True),\n",
        "                'subsample': trial.suggest_float('xgb_subsample', 0.5, 1.0),\n",
        "                'colsample_bytree': trial.suggest_float('xgb_colsample_bytree', 0.5, 1.0),\n",
        "                'gamma': trial.suggest_float('xgb_gamma', 1e-8, 1.0, log=True),\n",
        "                'min_child_weight': trial.suggest_int('xgb_min_child_weight', 1, 300),\n",
        "                'reg_alpha': trial.suggest_float('xgb_reg_alpha', 1e-8, 1.0, log=True),\n",
        "                'reg_lambda': trial.suggest_float('xgb_reg_lambda', 1e-8, 1.0, log=True),\n",
        "                'random_state': 42,\n",
        "                'n_jobs': -1\n",
        "            }\n",
        "            clf = XGBClassifier(**xgb_params)\n",
        "\n",
        "        return cross_val_score(clf, X, y, n_jobs=-1, cv=5).mean()\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(lambda trial: objective(trial=trial, X=X, y=y), n_trials=100)\n",
        "    trial = study.best_trial\n",
        "    return trial\n",
        "\n",
        "result = Optuna_Hyperparameter_tuning(X=X_train, y=y_train)\n",
        "print(f\"Accuracy: {result.value}\")\n",
        "print(f\"Best Hyperparameters: {result.params}\")"
      ],
      "metadata": {
        "id": "NhRJs1n63QOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model Using Startified K-Fold\n",
        "\n",
        "\n",
        "```\n",
        "# Perform 5-fold cross-validation with Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = cross_val_score(clf, X, y, cv=skf)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "7k3k2gs73cJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna For Logistic Regression\n",
        "\n",
        "import optuna\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "\n",
        "def LogisticRegression_Optuna(X, y):\n",
        "    def objective(trial, X, y):\n",
        "        params = {\n",
        "            'penalty' : trial.suggest_categorical('penalty', ['l1', 'l2']),\n",
        "            'C': trial.suggest_float('C', 1e-5, 1e5, log=True),\n",
        "            'solver': trial.suggest_categorical('solver', ['liblinear', 'saga'])\n",
        "        }\n",
        "\n",
        "        clf = LogisticRegression(**params)\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        return cross_val_score(clf, X, y, n_jobs=-1, cv=skf).mean()\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(lambda trial: objective(trial, X, y), n_trials=100)\n",
        "    trial = study.best_trial\n",
        "    return trial\n",
        "\n",
        "result = LogisticRegression_Optuna(X_train, y_train)\n",
        "print(f\"Logistic Regression Accuracy: {result.value}\")\n",
        "print(f\"Best Hyperparameters: {result.params}\")"
      ],
      "metadata": {
        "id": "03WSzqHl3Xep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna For XGBoost Stratified\n",
        "\n",
        "import optuna\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def XGB_Optuna(X, y):\n",
        "    def objective(trial, X, y):\n",
        "        params = {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "            'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
        "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n",
        "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
        "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1\n",
        "        }\n",
        "\n",
        "        clf = XGBClassifier(**params)\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        return cross_val_score(clf, X, y, n_jobs=-1, cv=skf).mean()\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(lambda trial: objective(trial=trial, X=X, y=y), n_trials=100)\n",
        "    trial = study.best_trial\n",
        "    return trial\n",
        "\n",
        "\n",
        "result = XGB_Optuna(X=X_train, y=y_train)\n",
        "print(f\"XGBoost Accuracy: {result.value}\")\n",
        "print(f\"Best Hyperparameters: {result.params}\")"
      ],
      "metadata": {
        "id": "LDLW0oUR3l6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna For KNeighborsClassifer\n",
        "\n",
        "import optuna\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def KNeighbors_Optuna(X, y):\n",
        "    def objective(trial, X, y):\n",
        "        params = {\n",
        "            'n_neighbors': trial.suggest_int('n_neighbors', 1, 30),\n",
        "            'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
        "            'algorithm': trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute']),\n",
        "            'leaf_size': trial.suggest_int('leaf_size', 10, 50),\n",
        "            'p': trial.suggest_int('p', 1, 2),\n",
        "            'n_jobs': -1\n",
        "        }\n",
        "\n",
        "        clf = KNeighborsClassifier(**params)\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        return cross_val_score(clf, X, y, n_jobs=-1, cv=skf).mean()\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(lambda trial: objective(trial=trial, X=X, y=y), n_trials=100)\n",
        "    trial = study.best_trial\n",
        "    return trial\n",
        "\n",
        "result = KNeighbors_Optuna(X=X_train, y=y_train)\n",
        "print(f\"KNeighborsClassifier Accuracy: {result.value}\")\n",
        "print(f\"Best Hyperparameters: {result.params}\")"
      ],
      "metadata": {
        "id": "h2rgf_HI3saG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}