{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YaMopiiM1m9k",
        "BhctHPh-NqHW",
        "XQDSWjatS-ZB",
        "zc4DEUs0dDH5",
        "e-U7R56u4GHi",
        "bhu-CPArSPn3",
        "xzEv0W2Ibl0M",
        "25NeeWGh8Zbx"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP2CDs1L+JIDvMKUGTSxGWs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RifatMuhtasim/Data_Science_Workflow/blob/main/4_3_Classification_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade nbformat nbconvert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh1R4QUuakjj",
        "outputId": "f49dd0c0-41c9-4bac-c0b6-424445dbbdb1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (5.10.4)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (6.5.4)\n",
            "Collecting nbconvert\n",
            "  Downloading nbconvert-7.16.4-py3-none-any.whl (257 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.4/257.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat) (4.19.2)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat) (5.7.2)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbformat) (5.7.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.7.1)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (3.1.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (2.1.5)\n",
            "Collecting mistune<4,>=2.0.3 (from nbconvert)\n",
            "  Downloading mistune-3.0.2-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert) (24.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (1.5.1)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (2.16.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (1.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert) (0.5.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (0.18.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.2.1)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.5.0->nbconvert) (6.1.12)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert) (2.5)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (24.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.8.2)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.3.3)\n",
            "Installing collected packages: mistune, nbconvert\n",
            "  Attempting uninstall: mistune\n",
            "    Found existing installation: mistune 0.8.4\n",
            "    Uninstalling mistune-0.8.4:\n",
            "      Successfully uninstalled mistune-0.8.4\n",
            "  Attempting uninstall: nbconvert\n",
            "    Found existing installation: nbconvert 6.5.4\n",
            "    Uninstalling nbconvert-6.5.4:\n",
            "      Successfully uninstalled nbconvert-6.5.4\n",
            "Successfully installed mistune-3.0.2 nbconvert-7.16.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Machine Learning Model"
      ],
      "metadata": {
        "id": "Mr75XQR0QgZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model for Classfication Problem: <br>\n",
        "1. Logistic Regression\n",
        "2. Decision Tree\n",
        "3. Random Forest\n",
        "4. Support Vector Machine (SVM)\n",
        "5. K-Nearest Neighbors (KNN)\n",
        "6. Naive Bayes\n",
        "8. XGBoost\n",
        "7. Deep Learning"
      ],
      "metadata": {
        "id": "BF_bPk401pr0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "YaMopiiM1m9k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3vpfcCy0w5n"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "lg_model = LogisticRegression()\n",
        "lg_model.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = lg_model.predict(X_train)\n",
        "y_val_pred = lg_model.predict(X_val)\n",
        "\n",
        "# Calculate Accuracy\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "print(\"Logistic Regression: \")\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Validation Accuracy:\", val_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ],
      "metadata": {
        "id": "BhctHPh-NqHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "decision_tree.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = decision_tree.predict(X_train)\n",
        "y_val_pred = decision_tree.predict(X_val)\n",
        "\n",
        "# Calculate Accuracy\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "print(\"Decision Tree: \")\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Validation Accuracy:\", val_accuracy)"
      ],
      "metadata": {
        "id": "qSpr_yjHNyqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "XQDSWjatS-ZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "random_forest = RandomForestClassifier()\n",
        "random_forest.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = random_forest.predict(X_train)\n",
        "y_val_pred = random_forest.predict(X_val)\n",
        "\n",
        "# Calculate Accuracy\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "print(\"Random Forest: \")\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Validation Accuracy:\", val_accuracy)"
      ],
      "metadata": {
        "id": "_A6XSAbkS_t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vectore Machine (SVM)"
      ],
      "metadata": {
        "id": "zc4DEUs0dDH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = svm.predict(X_train)\n",
        "y_val_pred = svm.predict(X_val)\n",
        "\n",
        "# Calculate Accuracy\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "print(\"Support Vector Machine: \")\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Validation Accuracy:\", val_accuracy)"
      ],
      "metadata": {
        "id": "-6XCkC51dJUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K Nearest Neighbors (KNN)"
      ],
      "metadata": {
        "id": "e-U7R56u4GHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = knn.predict(X_train)\n",
        "y_val_pred = knn.predict(X_val)\n",
        "\n",
        "# Calculate Accuracy\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "print(\"K-Nearest Neighbors: \")\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Validation Accuracy:\", val_accuracy)"
      ],
      "metadata": {
        "id": "QLb7z3Vy4FtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes"
      ],
      "metadata": {
        "id": "bhu-CPArSPn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gaussian Naive Bayes\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gaussian_naive_bayes = GaussianNB()\n",
        "gaussian_naive_bayes.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = gaussian_naive_bayes.predict(X_train)\n",
        "y_val_pred = gaussian_naive_bayes.predict(X_val)\n",
        "\n",
        "# Calculate Accuracy\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "print(\"Gaussian Naive Bayes: \")\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Validation Accuracy:\", val_accuracy)"
      ],
      "metadata": {
        "id": "cyUM7w1GSdzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multinomial Naive Bayes (Text Classification)\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample data (replace with your own dataset)\n",
        "X = [\"I love this movie\", \"This movie is boring\", \"The plot is great\", \"This movie is fantastic\"]\n",
        "y = [1, 0, 1, 1]  # 1 for positive sentiment, 0 for negative sentiment\n",
        "\n",
        "vectorized = CountVectorizer()\n",
        "X_vec = vectorized.fit_transform(X)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
        "\n",
        "mnb_classifier = MultinomialNB()\n",
        "mnb_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = mnb_classifier.predict(X_train)\n",
        "y_val_pred = mnb_classifier.predict(X_val)\n",
        "\n",
        "# Calculate Accuracy\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "print(\"Gaussian Naive Bayes: \")\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Validation Accuracy:\", val_accuracy)"
      ],
      "metadata": {
        "id": "0GzWHimeaTsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost"
      ],
      "metadata": {
        "id": "xzEv0W2Ibl0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "xgb = XGBClassifier()\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = xgb.predict(X_train)\n",
        "y_val_pred = xgb.predict(X_val)\n",
        "\n",
        "# Calculate Accuracy\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "print(\"XGBoost: \")\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Validation Accuracy:\", val_accuracy)"
      ],
      "metadata": {
        "id": "BEvuZk7ib_CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Neural Network"
      ],
      "metadata": {
        "id": "SMlCr65rnUtp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning"
      ],
      "metadata": {
        "id": "tT74WcTy2JAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "dl_model = Sequential([\n",
        "    Dense(64, input_shape=(X_train.shape[1], ), activation=\"relu\"),\n",
        "    Dense(32, activation=\"relu\"),\n",
        "    Dense(5, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "dl_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "print(dl_model.summary())"
      ],
      "metadata": {
        "id": "1eUWarZ62Khe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_fit, X_val_fit, y_train_fit, y_val_fit = train_test_split(X_train, y_train, stratify=y_train, test_size=0.20)\n",
        "\n",
        "history = dl_model.fit(X_train_fit, y_train_fit, validation_data=(X_val_fit, y_val_fit),  verbose=1, epochs=100)\n",
        "\n",
        "# Extract loss and accuracy values of the last epoch\n",
        "last_epoch_loss = history.history['loss'][-1]\n",
        "last_epoch_accuracy = history.history['accuracy'][-1]\n",
        "\n",
        "print(\"Last Epoch Loss:\", last_epoch_loss)\n",
        "print(\"Last Epoch Accuracy:\", last_epoch_accuracy)"
      ],
      "metadata": {
        "id": "dpQXD1YtEL9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Accuracy\n",
        "train_loss, train_accuracy = dl_model.evaluate(X_train, y_train)\n",
        "val_loss, val_accuracy = dl_model.evaluate(X_val, y_val)\n",
        "\n",
        "print(\"Deep Learning Model: \")\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Validation Accuracy:\", val_accuracy)"
      ],
      "metadata": {
        "id": "n_cvHGYZEYTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = dl_model.predict(X_val)\n",
        "y_pred = [np.argmax(i) for i in predictions]"
      ],
      "metadata": {
        "id": "OZyV3ZlvMnjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "25NeeWGh8Zbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "def Hyperparameter_tuning(X_train, y_train, model_params):\n",
        "  scores = []\n",
        "\n",
        "  for model, mp in model_params.items():\n",
        "    cv = GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n",
        "    cv.fit(X_train, y_train)\n",
        "    scores.append({\n",
        "      \"Model\": model,\n",
        "      \"Best_score\": cv.best_score_,\n",
        "      \"Best_params\": cv.best_params_\n",
        "    })\n",
        "\n",
        "  df = pd.DataFrame(scores, columns=['Model', 'Best_score', 'Best_params'])\n",
        "  return df"
      ],
      "metadata": {
        "id": "zMXkjH188dwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_params = {\n",
        "    'decision_tree':{\n",
        "        'model': DecisionTreeClassifier(),\n",
        "        'params': {\n",
        "            'max_depth': [None, 5, 10, 15, 20],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4],\n",
        "            'max_features': ['auto', 'sqrt', 'log2']\n",
        "        }\n",
        "    },\n",
        "    'random_forest': {\n",
        "        'model': RandomForestClassifier(),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [None, 5, 10, 15],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4],\n",
        "            'max_features': ['auto', 'sqrt', 'log2']\n",
        "        }\n",
        "    },\n",
        "    'xgb': {\n",
        "        'model': XGBClassifier(),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'learning_rate': [0.01, 0.1, 0.3],\n",
        "            'subsample': [0.8, 1.0],\n",
        "            'colsample_bytree': [0.8, 1.0]\n",
        "        }\n",
        "    },\n",
        "    'svm': {\n",
        "        'model': SVC(),\n",
        "        'params': {\n",
        "             'C': [0.1, 1, 10, 100],\n",
        "            'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "            'gamma': ['scale', 'auto']\n",
        "        }\n",
        "    },\n",
        "    'knn': {\n",
        "        'model': KNeighborsClassifier(),\n",
        "        'params': {\n",
        "            'n_neighbors': [3, 5, 7, 9, 11],\n",
        "            'weights': ['uniform', 'distance'],\n",
        "            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "            'p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
        "        }\n",
        "    },\n",
        "    'naive_bayes_gaussian' : {\n",
        "        'model': GaussianNB(),\n",
        "        'params': {}\n",
        "    },\n",
        "    'naive_bayes_multinomial' : {\n",
        "        'model':  MultinomialNB(),\n",
        "        'params': {\n",
        "            'alpha': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
        "        }\n",
        "    },\n",
        "    'logistic_regression': {\n",
        "        'model': LogisticRegression(),\n",
        "        'params': {\n",
        "            'penalty': ['l1', 'l2'],\n",
        "            'C': [0.01, 0.1, 1, 10, 100]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "results = Hyperparameter_tuning(X_train, y_train, model_params)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "results"
      ],
      "metadata": {
        "id": "CZov8IiC8mHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Hyperparameter Tuning for Neural Network"
      ],
      "metadata": {
        "id": "bF9GB2DpMrX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "import kerastuner as kt"
      ],
      "metadata": {
        "id": "WZv9_aF5Xlb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras import optimizers\n",
        "import keras_tuner as kt\n",
        "\n",
        "def Hyperparameter_tuning_for_neural_network(X_train, y_train, X_val, y_val):\n",
        "    def build_neural_model(hp):\n",
        "        model = Sequential()\n",
        "\n",
        "        counter = 0\n",
        "        for i in range(hp.Int('num_layers', min_value=1, max_value=10)):\n",
        "            if counter == 0:\n",
        "                model.add(\n",
        "                    Dense(\n",
        "                        units=hp.Int('units_' + str(i), min_value=8, max_value=128, step=8),\n",
        "                        activation=hp.Choice('activation_' + str(i), values=['relu', 'sigmoid', 'tanh']),\n",
        "                        input_shape=(X_train.shape[1],)\n",
        "                    )\n",
        "                )\n",
        "                model.add(\n",
        "                    Dropout(\n",
        "                        hp.Choice('dropout_' + str(i), values=[0.1, 0.2, 0.3])\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                model.add(\n",
        "                    Dense(\n",
        "                        units=hp.Int('units_' + str(i), min_value=8, max_value=128, step=8),\n",
        "                        activation=hp.Choice('activation_' + str(i), values=['relu', 'sigmoid', 'tanh'])\n",
        "                    )\n",
        "                )\n",
        "                model.add(\n",
        "                    Dropout(\n",
        "                        hp.Choice('dropout_' + str(i), values=[0.1, 0.2, 0.3])\n",
        "                    )\n",
        "                )\n",
        "            counter += 1\n",
        "\n",
        "        model.add(Dense(1, activation='sigmoid')) # softmax for 3 or more category\n",
        "\n",
        "        model.compile(optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd', 'nadam', 'adadelta']),\n",
        "                      loss='sparse_categorical_crossentropy',  # binary_crossentropy for binary classification\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    tuner = kt.RandomSearch(build_neural_model,\n",
        "                            objective='val_accuracy',\n",
        "                            max_trials=3,  # Update\n",
        "                            directory='mydir',\n",
        "                            project_name='final')\n",
        "\n",
        "    tuner.search(X_train, y_train, epochs=5, validation_data=(X_val, y_val))  # Update Epochs\n",
        "\n",
        "    # return tuner.get_best_models(num_models=1)[0]\n",
        "    return tuner.get_best_hyperparameters()[0].values"
      ],
      "metadata": {
        "id": "Wopsw2yYevMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = Hyperparameter_tuning_for_neural_network(X_train, y_train, X_val, y_val)\n",
        "params"
      ],
      "metadata": {
        "id": "GleTn3oFbVvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Parameters on Neural Network"
      ],
      "metadata": {
        "id": "spibXfvSndGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Define parameters\n",
        "params = {'num_layers': 6,\n",
        " 'units_0': 8,\n",
        " 'activation_0': 'tanh',\n",
        " 'dropout_0': 0.3,\n",
        " 'optimizer': 'sgd',\n",
        " 'units_1': 8,\n",
        " 'activation_1': 'relu',\n",
        " 'dropout_1': 0.1,\n",
        " 'units_2': 8,\n",
        " 'activation_2': 'relu',\n",
        " 'dropout_2': 0.1,\n",
        " 'units_3': 8,\n",
        " 'activation_3': 'relu',\n",
        " 'dropout_3': 0.1,\n",
        " 'units_4': 8,\n",
        " 'activation_4': 'relu',\n",
        " 'dropout_4': 0.1,\n",
        " 'units_5': 8,\n",
        " 'activation_5': 'relu',\n",
        " 'dropout_5': 0.1}\n",
        "\n",
        "# Create model\n",
        "model = Sequential()\n",
        "\n",
        "# Add layers\n",
        "for i in range(params['num_layers']):\n",
        "    if i == 0:\n",
        "        model.add(Dense(units=params[f'units_{i}'], activation=params[f'activation_{i}'] , input_shape=(X_train.shape[1], )))\n",
        "    else:\n",
        "        model.add(Dense(units=params[f'units_{i}'], activation=params[f'activation_{i}']))\n",
        "    model.add(Dropout(rate=params[f'dropout_{i}']))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=params['optimizer'], loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "76ISGQmamWeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "history = model.fit(X_train_fit, y_train_fit, epochs=100, batch_size=64, validation_data=(X_val_fit, y_val_fit))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_val, y_val)\n",
        "print(f'Validation Loss: {loss}, Validation Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "S6Mks_1MmVvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Intrepert the model Result"
      ],
      "metadata": {
        "id": "CmB13QY0c3Sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Generate the classification report using the true labels (y_test) and the predicted labels (y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "5xQY1SikdIys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "plt.figure(figsize=(12, 8))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")"
      ],
      "metadata": {
        "id": "Av8ovwB1c6dL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Final Model Training"
      ],
      "metadata": {
        "id": "BcGAAQDnib4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define logistic regression model with manually selected hyperparameters\n",
        "final_model = LogisticRegression(C=1, penalty='l2')\n",
        "\n",
        "# Train logistic regression model\n",
        "final_model.fit(X, y)"
      ],
      "metadata": {
        "id": "QbW869ErifR6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}